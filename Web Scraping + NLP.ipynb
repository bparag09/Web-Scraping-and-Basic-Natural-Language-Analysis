{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the input file to memory\n",
    "df = pd.read_excel('Input.xlsx', sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping 170 webpages using Beautifulsoup and requests modules to collect data\n",
    "\n",
    "\n",
    "###  timing the run. just over 4 minutes. Maybe we can reduce runtime if we establish a single session instead of making requests iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253.24933338165283\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "titles = []\n",
    "body_text = []\n",
    "\n",
    "for i in df.URL:\n",
    "    page_url = i\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/101.0.4951.67 Safari/537.36'}\n",
    "    html_text = requests.get(page_url, headers = headers).text\n",
    "\n",
    "    soup = bs(html_text, 'lxml')\n",
    "\n",
    "    title = soup.find('h1', class_ = 'entry-title').text\n",
    "    titles.append(title)\n",
    "\n",
    "    body_div = soup.find('div', class_ = 'td-post-content').text    \n",
    "    body_text.append(body_div)\n",
    "    \n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging the scraped text with input dataframe for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(titles)\n",
    "y = pd.Series(body_text)\n",
    "df1 = pd.DataFrame(x, columns = ['Title'])\n",
    "df2 = pd.DataFrame(y, columns = ['Body'])\n",
    "result = df.join(df1)\n",
    "result = result.join(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to dump extracted text to their respective URL_ID text files\n",
    "##  enter your own specific path of local disk for correct generation of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result.index:\n",
    "    url = result['URL_ID'][i]\n",
    "    # enter your own specific path of local disk for correct generation of files.\n",
    "    path = r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\New folder' + '\\\\' + str(url) + '.txt'\n",
    "    title = result.Title[i]\n",
    "    body = result.Body[i]\n",
    "    with open(path ,'w', encoding='utf-8') as f:\n",
    "        f.write(title)\n",
    "        f.write('\\n\\n')\n",
    "        f.writelines(body)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dumping dataframe with Title and Body to a csv file in disk for future analysis and avoiding webscraping again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('BlackCoffer.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading from the dumped csv above to avoid webscraping again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('BlackCoffer.csv')\n",
    "result.drop(columns = ['Unnamed: 0'],inplace=True)\n",
    "result = result.replace('\\n',' ', regex=True)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading stopwords and Master dictionary to memory for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\MasterDictionary\\negative-words.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    neg_words = []\n",
    "    for l in lines:\n",
    "        neg_words.append(l.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\MasterDictionary\\positive-words.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    pos_words = []\n",
    "    for l in lines:\n",
    "        pos_words.append(l.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\StopWords\\StopWords_Auditor.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    audit_stopword = []\n",
    "    for l in lines:\n",
    "        lst = l.split('|')\n",
    "        audit_stopword.append(lst[0].replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\StopWords\\StopWords_Currencies.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    curr_stopword = []\n",
    "    for l in lines:\n",
    "        lst = l.split('|')\n",
    "        curr_stopword.append(lst[0].replace(\"\\n\",\"\"))\n",
    "curr_stopword = list(map(lambda x: x.replace(\" \",\"\"), curr_stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\StopWords\\StopWords_DatesandNumbers.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    datenum_stopword = []\n",
    "    for l in lines:\n",
    "        lst = l.split('|')\n",
    "        datenum_stopword.append(lst[0].replace(\"\\n\",\"\"))\n",
    "datenum_stopword = list(map(lambda x: x.replace(\" \",\"\"), datenum_stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\StopWords\\StopWords_Generic.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    generic_stopword = []\n",
    "    for l in lines:\n",
    "        generic_stopword.append(l.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\StopWords\\StopWords_GenericLong.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    genlong_stopword = []\n",
    "    for l in lines:\n",
    "        genlong_stopword.append(l.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\StopWords\\StopWords_Geographic.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    geo_stopword = []\n",
    "    for l in lines:\n",
    "        lst = l.split('|')\n",
    "        geo_stopword.append(lst[0].replace(\"\\n\",\"\"))\n",
    "geo_stopword = list(map(lambda x: x.replace(\" \",\"\"), geo_stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\StopWords\\StopWords_Names.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    name_stopword = []\n",
    "    for l in lines:\n",
    "        lst = l.split('|')\n",
    "        name_stopword.append(lst[0].replace(\"\\n\",\"\"))\n",
    "name_stopword = list(map(lambda x: x.replace(\" \",\"\"), name_stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = audit_stopword+curr_stopword+datenum_stopword+generic_stopword+genlong_stopword+geo_stopword+name_stopword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dumping total Stopword and neg/pos words list to a file \n",
    "\n",
    "## Enter your own specific path of local disk drive for correct generation of output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\New folder\\stopword.txt', 'w') as f:\n",
    "    for i in stopword:\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\New folder\\pos_words.txt', 'w') as f:\n",
    "    for i in pos_words:\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n",
    "    f.close\n",
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\New folder\\neg_words.txt', 'w') as f:\n",
    "    for i in neg_words:\n",
    "        f.write(i)\n",
    "        f.write('\\n')\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading stopwords from above file to avoid reading them and making a total list again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\New folder\\stopword.txt', 'r') as f:\n",
    "    stopword = []\n",
    "    lst  = f.readlines()\n",
    "    for l in lst:\n",
    "        stopword.append(l.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\New folder\\pos_words.txt', 'r') as f:\n",
    "    pos_words = []\n",
    "    lst  = f.readlines()\n",
    "    for l in lst:\n",
    "        pos_words.append(l.replace(\"\\n\",\"\"))\n",
    "        \n",
    "with open(r'Z:\\Major Project\\assignment\\20211030 Test Assignment\\New folder\\neg_words.txt', 'r') as f:\n",
    "    neg_words = []\n",
    "    lst  = f.readlines()\n",
    "    for l in lst:\n",
    "        neg_words.append(l.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the case of stopwords to lower for easier analysis\n",
    "stopword = list(map(lambda x: x.lower(), stopword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to lowercase\n",
    "result['Title'] = result['Title'].map(lambda x: x.lower())\n",
    "result['Body'] = result['Body'].map(lambda x: x.lower())\n",
    "\n",
    "# Remove numbers\n",
    "result['Title'] = result['Title'].map(lambda x: re.sub(r'\\d+', '', x))\n",
    "result['Body'] = result['Body'].map(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "# Remove white spaces\n",
    "result['Title'] = result['Title'].map(lambda x: x.strip())\n",
    "result['Body'] = result['Body'].map(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine title and Body to analyse further\n",
    "result['Text'] = result['Title'].astype(str) + \" \" + result['Body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize into words\n",
    "result['Text'] = result['Text'].map(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non alphabetic tokens\n",
    "result['Text'] = result['Text'].map(lambda x: [word for word in x if word.isalpha()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The analysis listed below will be done before cleaning from stop words.\n",
    "## Analysis of Readability, complex word count, avg num of words per sentence, syllable count per word, personal pronoun, average word length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Syllable count per word and character length analysis according to text analysis document.\n",
    "lst_syllable = []\n",
    "lst_char = []\n",
    "for idx in result.index:\n",
    "    lst = []\n",
    "    char_lst = []\n",
    "    for i in result['Text'][idx]:\n",
    "        syllable_count=0\n",
    "        char_count = 0\n",
    "        for w in i:\n",
    "            if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u'):\n",
    "                syllable_count=syllable_count+1\n",
    "            char_count+=1\n",
    "        if (i[-1:-3:-1] == 'es' or i[-1:-3:-1] == 'ed'):\n",
    "            syllable_count -= 1\n",
    "        char_lst.append(char_count)\n",
    "        lst.append(syllable_count)\n",
    "    lst_syllable.append(lst)\n",
    "    lst_char.append(char_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting analyzing the text by storing the character length and syllable count per word in a dataframe: Syll_per_word\n",
    "x = pd.Series(lst_syllable)\n",
    "syll_per_word = pd.DataFrame(x, columns = ['Syllable per Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complex Word Count analysis\n",
    "syll_per_word['Complex Word Count'] = syll_per_word['Syllable per Word'].map(lambda x: [1 if syll>2 else 0 for syll in x])\n",
    "syll_per_word['Complex Word Count'] = syll_per_word['Complex Word Count'].map(lambda x: x.count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the body of the text into sentences.\n",
    "result['Sentences'] = result['Body'].map(lambda x: x.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the character length per word to the analysis dataframe.\n",
    "syll_per_word['char_length_per_word'] = pd.Series(lst_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg_word_length\n",
    "syll_per_word['Avg_word_length'] = syll_per_word['char_length_per_word'].map(lambda x: sum(x)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the total number of sentences into analysis dataframe.\n",
    "syll_per_word['total_num_of_sentences'] = result['Sentences'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg_num_words_per_sentence\n",
    "syll_per_word['Avg_num_words_per_sentence'] = syll_per_word['char_length_per_word'].map(lambda x: len(x))/syll_per_word['total_num_of_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Readability\n",
    "syll_per_word['avg_sentence_length'] = syll_per_word['char_length_per_word'].map(lambda x: len(x))/syll_per_word['total_num_of_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_per_word['percent_complex_words'] = syll_per_word['Complex Word Count']/syll_per_word['char_length_per_word'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_per_word['fog_index'] = 0.4*(syll_per_word['avg_sentence_length']+syll_per_word['percent_complex_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personal pronoun count\n",
    "lst_personal_pro = [] \n",
    "for idx in result.index:\n",
    "    str1 = result.Body[idx]\n",
    "    dict1 = {'i':0,'we':0,'my':0,'ours':0,'us':0}\n",
    "    r = re.compile(r'(?:\\bi\\b|\\bwe\\b|\\bmy\\b|\\bours\\b|\\bus\\b)')\n",
    "    for i in r.findall(str1):\n",
    "        if i=='we':dict1['we']+=1\n",
    "        if i=='i':dict1['i']+=1\n",
    "        if i=='my':dict1['my']+=1\n",
    "        if i=='ours':dict1['ours']+=1\n",
    "        if i=='us':dict1['us']+=1\n",
    "    lst_personal_pro.append(dict1)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_per_word['personal_pronoun'] = pd.Series(lst_personal_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning word list from total stopword list and using stopwords package of nltk and performing Sentiment analysis and cleaned Word count analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn lists back to string in order to remove punctuatuion as translate method doesnt work on a list object\n",
    "result['Text'] = result['Text'].map(lambda x: ' '.join(x))\n",
    "# Remove Punctuation\n",
    "result['Text']  = result['Text'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n",
    "# Tokenize again in order to filter out stop words from nltk package\n",
    "result['Text'] = result['Text'].map(lambda x: word_tokenize(x))\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "result['Text'] = result['Text'].map(lambda x: [w for w in x if not w in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning text from total stopword list given in the stopword folder:\n",
    "result['Text'] = result['Text'].map(lambda x: [w for w in x if not w in stopword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "syll_per_word['word_count'] = result['Text'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis:\n",
    "syll_per_word['positive_score'] = result['Text'].map(lambda x: sum([1 for w in x if w in pos_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_per_word['negative_score'] = result['Text'].map(lambda x: sum([1 for w in x if w in neg_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_per_word['polarity_score'] = (syll_per_word['positive_score'] - syll_per_word['negative_score'])/((syll_per_word['positive_score'] + syll_per_word['negative_score'])+0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_per_word['subjectivity_score'] = (syll_per_word['positive_score'] + syll_per_word['negative_score'])/(syll_per_word['word_count']+ 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dumping the analysis dataframe to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_per_word.to_csv('unfinal.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generating proper output file and dumping to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = syll_per_word[['positive_score','negative_score','polarity_score','subjectivity_score','avg_sentence_length','percent_complex_words', 'fog_index', 'Avg_num_words_per_sentence', 'Complex Word Count', 'word_count', 'Syllable per Word', 'personal_pronoun', 'Avg_word_length' ]]\n",
    "df2 = result[['URL_ID','URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the analysis dataframe and input dataframe to generate the output dataframe.\n",
    "unfinal = df2.join(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns for proper representation.\n",
    "dict = {'positive_score': 'POSITIVE SCORE',\n",
    "        'negative_score': 'NEGATIVE SCORE',\n",
    "        'polarity_score': 'POLARITY SCORE',\n",
    "        'subjectivity_score': 'SUBJECTIVITY SCORE',\n",
    "        'avg_sentence_length': 'AVG SENTENCE LENGTH',\n",
    "        'percent_complex_words': 'PERCENTAGE OF COMPLEX WORDS',\n",
    "        'fog_index': 'FOG INDEX',\n",
    "        'Avg_num_words_per_sentence': 'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "        'Complex Word Count': 'COMPLEX WORD COUNT',\n",
    "        'word_count': 'WORD COUNT',\n",
    "        'Syllable per Word': 'SYLLABLE PER WORD',\n",
    "        'personal_pronoun': 'PERSONAL PRONOUNS',\n",
    "        'Avg_word_length': 'AVG WORD LENGTH'}\n",
    " \n",
    "final = unfinal.rename(columns=dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dumping the final output to disk for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_excel ('Output Data Structure.xlsx', index = None, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
